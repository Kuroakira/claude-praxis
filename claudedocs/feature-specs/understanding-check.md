# FeatureSpec: Understanding Check

## Problem Statement

AI が成果物（コード、Design Doc、FeatureSpec）を書いてくれる現在のワークフローでは、エンジニアは成果物を「なんとなく眺めて」承認してしまいがちである。自分では理解したつもりでいるが、チームメンバーから「なぜこのコードにしたの？」「なぜこの設計にしたの？」と質問されて初めて、理解の穴に気づく。

現状の claude-praxis は質の高い成果物を作るプロセスは充実しているが、**エンジニア自身がその成果物を自分の言葉で説明できるかを検証する仕組み**がない。知識の不足ではなく、理解を試す機会の不在が問題の本質である。

### 既存メカニズムとの差分

claude-praxis には既に理解を促す仕組みがある：

- `check-past-learnings`: 過去の判断の rationale を提示し「同じ前提が成り立つか？」を問う
- `/implement` の decision points: 実装中に複数の選択肢をエンジニアに提示する
- `/compound`: 実装後に学びを振り返り、蓄積する

これらは「判断の機会を与える」「振り返りを促す」メカニズムであり、**エンジニアが成果物を自分の言葉で説明できるかを直接検証するもの**ではない。decision points で選択した判断でも、その理由を後から説明できるとは限らない。Understanding Check はこの残存ギャップ — 「選んだけど説明できない」状態 — を対象とする。

## User Stories

### チェックの開始 — 理解度確認を始める

各ワークフロー（`/feature-spec`, `/design`, `/implement`, `/debug`）のドラフト完了後に「理解度チェックしますか？」と提案される。エンジニアは成果物の規模や時間の余裕に応じて受けるかスキップするかを判断する。小さなバグ修正や設定変更など、チェックの価値が薄い場合は自然にスキップし、設計判断を伴う作業では受ける。

別セッションからの実行にも対応する。新しいセッションで `/understanding-check` を実行すると、progress.md + 成果物（+ git diff）から文脈を復元し、質問を生成する。実装直後ではなく間隔を空けて受けることで、短期記憶ではなく本質的な理解を検証できる（spacing effect）。

### 説明チャレンジ — 自分の言葉で説明する

チェックを受けると、成果物作成中の主要な判断について質問が提示される。質問の対象はフェーズによって変わる：

- **FeatureSpec**: 「なぜこのスコープにした？」「なぜこのユーザーストーリーを選んだ？」
- **Design Doc**: 「なぜこのアーキテクチャ？」「なぜ代替案を却下した？」
- **コード**: 「なぜこのパターン？」「そもそもXXXとは何？」
- **Debug**: 「なぜこの原因だと特定した？」「他の可能性をどう排除した？」

エンジニアはまず **自分の言葉で** 回答を試みる。AI の解説を見る前に自分で言語化することで、理解できている部分とできていない部分が明確になる。

### ギャップ発見と深掘り — 理解の穴を埋める

自分の説明を出した後、AI が作成時の rationale を提示して比較する。説明が的確だった箇所は理解を確認できた証拠になる。説明できなかった箇所や、AI の rationale と大きく乖離していた箇所は理解のギャップとして可視化される。エンジニアはギャップについて AI に質問して理解を深めることができる。発見されたギャップは progress.md に記録され、`/compound` で学びとして蓄積される。

## Scope

### In Scope

Understanding Check は、各ワークフローのドラフト完了後に opt-in で実行される理解度確認機能である。エンジニアが自分の言葉で判断を説明し、AI の rationale との比較を通じてギャップを発見する。

- 全ワークフロー（`/feature-spec`, `/design`, `/implement`, `/debug`）のドラフト完了後に提案される理解度チェック
- 別セッションからの単体実行（`/understanding-check`）。progress.md + 成果物 + git diff から文脈を復元
- 各フェーズの判断に基づく質問の生成（設計判断・実装判断・スコープ判断・概念理解など）
- 「エンジニアが先に説明 → AI の rationale と比較」の生成ベース学習構造
- opt-in 方式（提案するが強制しない）
- 理解ギャップの progress.md への記録（`/compound` との統合）
- 完了レポートへの理解確認ステータス記載（「主要判断 5件中 3件を説明可能」等）
- 各コマンドのドラフト完了時に progress.md へ主要判断を書き込む仕組みの整備（Understanding Check の文脈復元に必要な前提基盤）
- 質問の出し方は運用しながら調整する前提の設計

### Out of Scope

- 理解度の長期スコアリングや履歴管理システム（完了レポートへの記載と progress.md への記録で代替）
- LMS 的な学習管理機能（進捗トラッキング、カリキュラム設計など）
- チーム全体への展開を前提とした機能（まずは個人利用）

## Purpose

Understanding Check の目的は、AI が作成した成果物に対するエンジニアの「理解したつもり」を、承認前に検証することである。成功の状態：

- **理解の穴の事前特定**: チームレビュー前に、自分が説明できない箇所を自覚できている
- **説明可能な状態での承認**: 成果物を承認する時点で、チームメンバーに「なぜこうしたのか」を自分の言葉で説明できる
- **当事者意識の回復**: AI が作成した成果物であっても、判断を自分のものとして語れる
- **学びの蓄積**: 理解できなかった箇所が `/compound` に流れ、次回以降の判断の糧になる

## Risks

- **質問の質が低いと形骸化する**: 表面的な質問ばかりだとチェックが作業になり、理解を深める効果がない。各コマンドがドラフト完了時に progress.md に記録した主要判断を質問の素材とすることで、実際の判断に根ざした質問を生成する方針とする
- **面倒になって常にスキップされる**: opt-in にすると使われなくなるリスク。対策として (1) 完了レポートに理解確認ステータスを記載し可視性を持たせる、(2) 質問数を最小限（3〜5問）に抑えて摩擦を下げる、(3) ギャップが `/compound` に流れることで後続の学びに価値が生まれる導線を作る
- **AI の sycophancy リスク**: エンジニアの回答を安易に「正解」と判定すると、理解の illusion を強化してしまう。回答の評価では、既存の anti-sycophancy パターン（`receiving-code-review` skill）を参照し、欠落している観点や不正確な部分を明示的に指摘する設計とする
- **コンテキストウィンドウの圧迫**: ワークフロー完了後のセッションは既にコンテキストが大きい可能性がある。別セッションでの `/understanding-check` 単体実行を推奨し、質問数の上限（3〜5問）で対応する。別セッション実行には spacing effect による学習効果の向上という副次的メリットもある

## References

- claude-praxis コアコンセプト: 「Bring understanding back to vibe coding — by articulating and accumulating the 'why' behind design and implementation decisions」
- Illusion of Explanatory Depth (Rozenblit & Keil, 2002): 人は複雑なシステムへの理解を過大評価する傾向があり、説明を求められて初めてギャップに気づく。Understanding Check はこの現象を意図的に引き起こす仕組み
- 生成ベース学習: 教育心理学において、情報を受動的に読むより自分で生成（言語化）する方が長期記憶に定着しやすい。「AI の解説を読んでから再テスト」ではなく「まず自分で説明」を採用した根拠
- Spacing Effect: 間隔を空けた復習は直後の復習より長期記憶への定着率が高い。別セッションでの実行を推奨する根拠
